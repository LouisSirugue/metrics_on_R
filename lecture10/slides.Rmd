---
title: "Inference"
subtitle: "Lecture 10"
author: "<br>Louis SIRUGUE"
date: "CPES 2 - Fall 2022"
params:
  dark: false
output:
  xaringan::moon_reader:  
    css: 
        - xaringan-themer.css
        - theme.css
    includes:
      in_header: header.html
      after_body: insert-logo.html
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

<style> .left-column {width: 70%;} .right-column {width: 30%;} </style>

```{css, echo = F, eval = params$dark}
body{background-color:black;filter:invert(1)}
```

```{r setup, include = FALSE}
source(paste0(getwd(), "/../source/style.R"))
stargazer <- stargazer::stargazer
theme_minimal <- theme_Rcourse
options(htmltools.dir.version = F)
knitr::opts_chunk$set(echo = T, message = F, warning = F, fig.align = "center", dpi = 300, out.width = "100%")
set.seed(1)
```

### Quick reminder

#### 1. Regression

.pull-left[
<p style = "margin-bottom:-.75cm;"></p>
```{r, echo = F, fig.width = 5.5, fig.height = 3.5, out.width = "100%"}
ggcurve <- read.csv("ggcurve.csv")

ols <- summary(lm(ige ~ gini, ggcurve))$coefficients

attempts <- ggcurve %>%
  mutate(`Attempt 3` = ols[1, 1] + (gini * ols[2, 1]),
         `Attempt 2` = (3 * gini) - 1,
         `Attempt 1` = .45) %>%
  pivot_longer(c(`Attempt 1`, `Attempt 2`, `Attempt 3`), names_to = "line_id", values_to = "line_value")

test <- attempts %>%
  mutate(obs = row_number()) %>%
  pivot_longer(c(ige, line_value), names_to = "dot_id", values_to = "dot")

attempts %>%
  filter(line_id == "Attempt 3") %>%
  ggplot(., aes(x = gini, y = ige)) + 
  geom_point(alpha = .4, color = "#014D64") +
  geom_line(aes(y = line_value), alpha = .8, color = "#014D64") +
  geom_line(data = test %>% filter(line_id == "Attempt 3"), 
  aes(x = gini, y = dot, group = obs), linetype = "dashed", alpha = .7, color = "#014D64") +
  xlab("x") + ylab("y")
```
<p style = "margin-bottom:-1.2cm;"></p>
```{r, echo = F}
data <- ggcurve %>%
  rename(x = gini, y = ige)
lm(y ~ x, data)
```
]

--

.pull-right[

 * This can be expressed with the **regression equation:**
 
$$y_i = \hat{\alpha} + \hat{\beta}x_i + \hat{\varepsilon_i}$$


 * Where $\hat{\alpha}$ is the **intercept** and $\hat{\beta}$ the **slope** of the **line** $\hat{y_i} = \hat{\alpha} + \hat{\beta}x_i$, and $\hat{\varepsilon_i}$ the **distances** between the points and the line

<p style = "margin-bottom:1cm;">

$$\hat{\beta} = \frac{\text{Cov}(x_i, y_i)}{\text{Var}(x_i)}$$ 

$$\hat{\alpha} = \bar{y} - \hat{\beta} \times\bar{x}$$

* $\hat{\alpha}$ and $\hat{\beta}$ minimize $\hat{\varepsilon_i}$

]

---

### Quick reminder

#### 2. Multivariate regressions

<ul>
  <li><b>Adding</b> a second independent <b>variable</b> in the regression amounts to <b>fitting a plane</b> instead of a line</li>
  <ul>
    <li>Adding a third variable would fit a hyperplane of dimension 3 and so on</li>
  </ul>
</ul>

--

.pull-left[

<center><b>Adding a continuous variable</b></center>

```{r, echo = F, fig.width = 6, fig.height = 1.25, out.width= '100%'}
library(plotly)
library(reshape2)

ggcurve <- ggcurve %>%
  mutate(third_variable = (4 * gini)  + rnorm(nrow(.), 0, 1))

petal_lm <- lm(ige ~ gini + third_variable, ggcurve)
graph_reso <- 0.05

#Setup Axis
axis_x <- seq(min(ggcurve$gini), max(ggcurve$gini), by = graph_reso)
axis_y <- seq(min(ggcurve$third_variable), max(ggcurve$third_variable), by = graph_reso)

#Sample points
petal_lm_surface <- expand.grid(gini = axis_x, third_variable = axis_y, KEEP.OUT.ATTRS = F)
petal_lm_surface$ige <- predict.lm(petal_lm, newdata = petal_lm_surface)
petal_lm_surface <- acast(petal_lm_surface, third_variable ~ gini, value.var = "ige") #y ~ x

plot_ly(ggcurve, x = ~gini, y = ~third_variable, z = ~ige) %>% 
  add_markers(opacity = .8) %>%
  add_trace(z = petal_lm_surface,
            x = axis_x,
            y = axis_y,
            type = "surface", 
            colorscale ='YlGnBu',
            opacity = .8) %>% 
  layout(plot_bgcolor = "#DFE6EB", paper_bgcolor = "#DFE6EB", fig_bgcolor = "#DFE6EB", xaxis = list(zeroline = F),
    yaxis = list(zeroline = F),
         aspectratio = list(x = 2, y = 1, z = .5),
         scene = list(xaxis = list(title = 'Gini'),
                      yaxis = list(title = 'Third variable'),
                      zaxis = list(title = 'IGE'),
                      camera = list(eye = list(x = -.75, 
                                               y = -2, 
                                               z = .75)))) %>% 
  style(hoverinfo = 'skip')
```
]

.pull-right[

<center><b>Adding a discrete variable</b></center>

```{r, echo = F, fig.width = 6, fig.height = 1.25, out.width= '100%'}
asec <- read.csv("asec.csv") %>%
  mutate(Black = as.numeric(Race == "Black"),
         Other = as.numeric(Race == "Other"),
         `Log Earnings` = log(Earnings))

petal_lm <- lm(`Log Earnings` ~ Black + Other, asec)
graph_reso <- 0.05

#Setup Axis
axis_x <- seq(min(asec$Black), max(asec$Black), by = graph_reso)
axis_y <- seq(min(asec$Other), max(asec$Other), by = graph_reso)

#Sample points
petal_lm_surface <- expand.grid(Black = axis_x, Other = axis_y, KEEP.OUT.ATTRS = F)
petal_lm_surface$`Log Earnings` <- predict.lm(petal_lm, newdata = petal_lm_surface)
petal_lm_surface <- acast(petal_lm_surface, Other ~ Black, value.var = "Log Earnings") #y ~ x

plot_ly(asec, x = ~Black, y = ~Other, z = ~`Log Earnings`) %>% 
  add_markers(opacity = .005) %>%
  add_trace(z = petal_lm_surface,
            x = axis_x,
            y = axis_y,
            type = "surface", 
            colorscale ='YlGnBu',
            opacity = .8) %>% 
  layout(plot_bgcolor = "#DFE6EB", paper_bgcolor = "#DFE6EB", fig_bgcolor = "#DFE6EB",
         aspectratio = list(x = 2, y = 1, z = .5),
         scene = list(xaxis = list(title = 'Black'),
                      yaxis = list(title = 'Other'),
                      zaxis = list(title = 'Log Earnings'),
                      camera = list(eye = list(x = -.75, 
                                               y = -2, 
                                               z = .75)))) %>% 
  style(hoverinfo = 'skip')
```

]

---

### Quick reminder

#### 3. Control variables

<ul>
  <li>Adding a third variable \(z\) <b>removes</b> its potential <b>confounding effect</b> from the relationship between \(x\) and \(y\)</li>
  <ul>
    <li>As we move along the \(x\) axis, the <b>third variable remains constant</b></li>
  </ul>
</ul>

--

$$\hat{y_i} = \hat{\alpha} + \hat{\beta_1} x + \hat{\beta_2} z + \hat{\varepsilon_i}$$

--

```{r, echo = F, fig.width = 7, fig.height = 4, out.width = "55%"}
library(gganimate)

discrete <- tibble(Hours = c(rnorm(500, 35, 5), rnorm(500, 30, 5)),
                   Sex = rep(c("Male", "Female"), each = 500),
                   Earnings = 12.5 * Hours + ifelse(Sex == "Male", rnorm(500, 1200, 100), rnorm(500, 1000, 100)))

coefs <- summary(lm(Earnings ~ Hours + Sex, discrete))$coefficients
anim_discrete <- discrete %>%
  mutate(time = 0,
         coef_1 = coefs[1, 1],
         coef_2 = coefs[2, 1],
         coef_3 = coefs[3, 1])

for (i in 1:100) {
  
  temp_discrete <- discrete %>%
  group_by(Sex) %>%
  mutate(Hours = ifelse(Sex == "Male", Hours - (i/100) * mean(Hours), Hours))
         #Earnings = Earnings - (i/100) * mean(Earnings))
  
  coefs <- summary(lm(Earnings ~ Hours + Sex, temp_discrete))$coefficients
  
  anim_discrete <- anim_discrete %>%
    bind_rows(temp_discrete %>%
  mutate(time = i,
         coef_1 = coefs[1, 1],
         coef_2 = coefs[2, 1],
         coef_3 = coefs[3, 1]))
} 

discrete2 <- discrete %>%
  group_by(Sex) %>%
  mutate(Hours = ifelse(Sex == "Male", Hours - mean(Hours), Hours))

for (j in 1:100) {
  
  temp_discrete <- discrete2 %>%
  group_by(Sex) %>%
  mutate(Earnings = ifelse(Sex == "Male", Earnings - (j/100) * mean(Earnings), Earnings))
  
  coefs <- summary(lm(Earnings ~ Hours + Sex, temp_discrete))$coefficients
  
  anim_discrete <- anim_discrete %>%
    bind_rows(temp_discrete %>%
  mutate(time = j + 100,
         coef_1 = coefs[1, 1],
         coef_2 = coefs[2, 1],
         coef_3 = coefs[3, 1]))
} 

discrete3 <- discrete2 %>%
  group_by(Sex) %>%
  mutate(Earnings = ifelse(Sex == "Male", Earnings - mean(Earnings), Earnings))

for (j in 1:100) {
  
  temp_discrete <- discrete3 %>%
  group_by(Sex) %>%
  mutate(Hours = ifelse(Sex == "Female", Hours - (j/100) * mean(Hours), Hours))
  
  coefs <- summary(lm(Earnings ~ Hours + Sex, temp_discrete))$coefficients
  
  anim_discrete <- anim_discrete %>%
    bind_rows(temp_discrete %>%
  mutate(time = j + 200,
         coef_1 = coefs[1, 1],
         coef_2 = coefs[2, 1],
         coef_3 = coefs[3, 1]))
} 

discrete4 <- discrete3 %>%
  group_by(Sex) %>%
  mutate(Hours = ifelse(Sex == "Female", Hours - mean(Hours), Hours))

for (j in 1:100) {
  
  temp_discrete <- discrete4 %>%
  group_by(Sex) %>%
  mutate(Earnings = ifelse(Sex == "Female", Earnings - (j/100) * mean(Earnings), Earnings))
  
  coefs <- summary(lm(Earnings ~ Hours + Sex, temp_discrete))$coefficients
  
  anim_discrete <- anim_discrete %>%
    bind_rows(temp_discrete %>%
  mutate(time = j + 300,
         coef_1 = coefs[1, 1],
         coef_2 = coefs[2, 1],
         coef_3 = coefs[3, 1]))
} 


limits <- discrete %>%
  group_by(Sex) %>%
  summarise(earn = min(Earnings) - mean(Earnings),
            hours = min(Hours) - mean(Hours)) %>%
  ungroup() %>%
  summarise(earn = min(earn), hours = min(hours))

data_segment_female <- anim_discrete %>%
  filter(Sex == "Female") %>%
  group_by(time) %>%
  summarise(minhours = min(Hours),
            maxhours = max(Hours),
            y = mean(coef_1) + minhours * mean(coef_2),
            yend = mean(coef_1)  + maxhours *  mean(coef_2))

data_segment_male <- anim_discrete %>%
  filter(Sex == "Male") %>%
  group_by(time) %>%
  summarise(minhours = min(Hours),
            maxhours = max(Hours),
            y = mean(coef_1)  +  mean(coef_3) + (minhours *  mean(coef_2)), 
            yend = mean(coef_1)  +  mean(coef_3)  + (maxhours *  mean(coef_2)))


test <- ggplot(anim_discrete, aes(x = Hours, y = Earnings, color = Sex)) + 
  # 0xy
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  # Data
  geom_point(alpha = .1) +
  geom_segment(data = data_segment_female,
               aes(x = minhours,xend = maxhours, y = y, yend = yend),
               size = 1, color = "#6794A7", alpha = .1)+
  geom_segment(data = data_segment_male,
               aes(x = minhours, xend = maxhours, y = y, yend = yend),
               size = 1, color = "#00A2D9", alpha = .1) + 
  # Scales
  scale_y_continuous(limits = c(limits$earn, 2050)) +
  scale_x_continuous(limits = c(limits$hours, max(discrete$Hours))) +
  transition_time(time) +
  # Animation
  ease_aes("linear") + 
  shadow_wake(1/100, size = 1, alpha = TRUE, wrap = FALSE, #exclude_layer = c(2, 3),
              falloff = 'sine-in', exclude_phase = 'enter')

animate(test, nframes = 400, fps = 30, end_pause = 30, height = 800, width = 1400, res = 200)
```

---

### Quick reminder

#### 4. Interactions

<ul>
  <li>Adding an <b>interaction</b> term with \(z\) allows to see <b>how the effect</b> of \(x\) on \(y\) <b>varies</b> with \(z\)</li>
  <ul>
    <li>If \(z\) is <b>discrete</b>, it amounts to <b>regressing</b> \(y\) on \(x\) <b>separately</b> for each \(z\) group</li>
  </ul>
</ul>

--

$$\hat{y_i} = \hat{\alpha} + \hat{\beta_1} x + \hat{\beta_2} z + \hat{\beta_3}(x \times z)+ \hat{\varepsilon_i}$$

--

```{r, echo = F, fig.width = 7, fig.height = 4, out.width = "55%"}
edu <- read.csv("household_data.csv")

ggplot(edu, aes(x = Income, y = Children, color = Education, shape = Education)) +
  geom_point(alpha = .6, size = 3) +
  geom_smooth(method = "lm", formula = y ~ x, se = F, show.legend = F)
```

---

<h3>Today: Inference</h3>

--

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

.pull-right[

<ul style = "margin-left:-1cm;list-style: none">
  <li><b>3. Hypothesis testing</b></li>
  <ul style = "list-style: none">
    <li>3.1. P-value</li>
    <li>3.2. linearHypothesis()</li>
  </ul>
</ul>
 
<p style = "margin-bottom:1.75cm;"></p>

<ul style = "margin-left:-1cm;list-style: none"><li><b>4. Wrap up!</b></li></ul>
]

---

<h3>Today: Inference</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

]

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>In Part I of the course, we distinguished the <b>empirical moments</b> from the <b>theoretical moments</b></li>
  <ul>
    <li>Like the <i>empirical mean</i> is a <b>finite-sample estimation</b> of the <i>theoretical expected value</i></li>
    <li>The same principle applies to <b>regression coefficients</b></li>
  </ul>
</ul>

--

<ul>
  <li>Take our Great Gatsby Curve for instance</li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>

.left-column[
<p style = "margin-bottom:-.5cm;"></p>
```{r, echo = F, fig.height = 3.375, fig.width = 6, out.width="65%"}
coefs <- summary(lm(ige ~ gini, ggcurve))$coefficients
ggcurve <- ggcurve %>%
  mutate(line_value = coefs[1, 1] + gini * coefs[2, 1])

ggplot(ggcurve, aes(x = gini, y = ige, label = country)) +
  geom_text(alpha = .8, color = "#014D64") +
  geom_line(aes(y = line_value), alpha = .8, color = "#014D64") 
```
]

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>In Part I of the course, we distinguished the <b>empirical moments</b> from the <b>theoretical moments</b></li>
  <ul>
    <li>Like the <i>empirical mean</i> is a <b>finite-sample estimation</b> of the <i>theoretical expected value</i></li>
    <li>The same principle applies to <b>regression coefficients</b></li>
  </ul>
</ul>

<ul>
  <li>Take our Great Gatsby Curve for instance</li>
  <ul>
    <li>Had our <b>sample</b> of countries been a bit <b>different</b>, our <b>coefficients</b> would <b>not be the same</b></li>
    <li></li>
  </ul>
</ul>

.left-column[
<p style = "margin-bottom:-.5cm;"></p>
```{r, echo = F, fig.height = 3.375, fig.width = 6, out.width="65%"}
ggcurve <- ggcurve %>%
  mutate(grouping1 = ifelse(country %in% c("Canada", "Chile", "Italy", "Sweden", "Japan"), "Hide", "Show"))

coefs <- summary(lm(ige ~ gini, ggcurve %>% filter(grouping1 == "Show")))$coefficients
ggcurve <- ggcurve %>%
  mutate(line_value2 = coefs[1, 1] + gini * coefs[2, 1])

ggplot(ggcurve, aes(x = gini, y = ige, label = country, alpha = grouping1)) +
  geom_text(color = "#014D64", show.legend = F) +
  geom_line(aes(y = line_value), alpha = .2, color = "#014D64")  +
  geom_line(aes(y = line_value2), alpha = .8, color = "#014D64") +
  scale_alpha_manual(values = c(.3, .8))
```
]

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>In Part I of the course, we distinguished the <b>empirical moments</b> from the <b>theoretical moments</b></li>
  <ul>
    <li>Like the <i>empirical mean</i> is a <b>finite-sample estimation</b> of the <i>theoretical expected value</i></li>
    <li>The same principle applies to <b>regression coefficients</b></li>
  </ul>
</ul>

<ul>
  <li>Take our Great Gatsby Curve for instance</li>
  <ul>
    <li>Had our <b>sample</b> of countries been a bit <b>different</b>, our <b>coefficients</b> would <b>not be the same</b></li>
    <li>But they would all be <b>estimations</b> of a true relationship whose <b>data-generating</b> process is <b>unobserved</b></li>
  </ul>
</ul>

.left-column[
<p style = "margin-bottom:-.5cm;"></p>
```{r, echo = F, fig.height = 3.375, fig.width = 6, out.width="65%"}
ggcurve <- ggcurve %>%
  mutate(grouping2 = ifelse(country %in% c("Peru", "Australia", "Denmark", "United Kingdom", "Japan"), "Hide", "Show"))

coefs <- summary(lm(ige ~ gini, ggcurve %>% filter(grouping2 == "Show")))$coefficients
ggcurve <- ggcurve %>%
  mutate(line_value3 = coefs[1, 1] + gini * coefs[2, 1])

ggplot(ggcurve, aes(x = gini, y = ige, label = country, alpha = grouping2)) +
  geom_text(color = "#014D64", show.legend = F) +
  geom_line(aes(y = line_value), alpha = .2, color = "#014D64")  +
  geom_line(aes(y = line_value2), alpha = .2, color = "#014D64") +
  geom_line(aes(y = line_value3), alpha = .8, color = "#014D64") +
  scale_alpha_manual(values = c(.3, .8))
```
]

--

.right-column[
<p style = "margin-bottom:2cm;"></p>
<center><b>Then how to asses the reliability of our estimation?</b></center>
]


---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>For <b>simplicity</b>, let's work with a relationship whose <b>DGP is know</b></li>
  <ul>
    <li>Such that we can <b>understand how estimations</b> from random samples <b>behave relative to the DGP</b></li>
    <li>Let's <b>generate data in R!</b></li>
  </ul>
</ul>

--

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>We can use <b>functions</b> that output <b>random draws from given distributions</b> whose parameters can be chosen</li>
</ul>

--

<p style = "margin-bottom:1.25cm;"></p>

.pull-left[
<center><b>Normal distribution</b></center>

&#10140; Sample size, expected value, standard deviation
  
```{r, eval = F}
rnorm(n = 10, mean = 100, sd = 5)
```

```{r, eval = F, highlight=F}
##  [1] 103.48482 102.78332  96.55622
##  [4]  96.46252 101.82291 103.84266
##  [7]  99.43827 104.40554 101.99053
## [10]  96.93987
```

]

.pull-right[
<center><b>Uniform distribution</b></center>

&#10140; Sample size, lower bound, upper bound

```{r, eval=F}
runif(n = 10, min = 4, max = 5)
```

```{r, eval = F, highlight=F}
##  [1] 4.633493 4.213208 4.129372
##  [4] 4.478118 4.924074 4.598761
##  [7] 4.976171 4.731793 4.356727
## [10] 4.431474
```
]

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>Consider the following <b>data generating process:</b></li>
</ul>

$$y = -2 + 0.4  \times x + \varepsilon \:\:\:\begin{cases}
x \sim \mathcal{N}(4,\,25)\\
\varepsilon \sim \mathcal{N}(0,\,1)
\end{cases}$$

--

<ul>
  <li>We can randomly draw <b>1,000 observations</b from this DGP as follows</li>
</ul>

```{r}
dt <- tibble(x = rnorm(1000, 4, 5),
             e = rnorm(1000, 0, 1),
             y = -2 + (.4 * x) + e)
```

--

<p style = "margin-bottom:1cm"></p>

<center><b>Check the empirical moments:</b></center>

.pull-left[
```{r}
c(mean(dt$x), var(dt$x))
```
]

.pull-right[
```{r}
c(mean(dt$e), var(dt$e))
```
]

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>Because the randomly drawn <b>sample is finite</b>, it <b>does not match exactly</b> the features of the DGP:</li>
</ul>

--

```{r, echo = F, fig.height = 4.75, fig.width = 10, out.width="85%"}
dt %>%
  bind_rows(tibble(x = -10, e = -10, y = NA_real_)) %>%
  mutate(Distribution = ifelse(is.na(y), "DGP", "Sample")) %>%
  pivot_longer(c(x, e), names_to = "Variable", values_to = "val") %>%
  ggplot(aes(x = val)) + geom_density(aes(color = Distribution), size = 1, alpha = .6) +
  geom_line(data = tibble(val = c(seq(-10, 20, .1), seq(-10, 15, .1)),
                          Variable = c(rep("x", length(seq(-10, 20, .1))), rep("e", length(seq(-10, 15, .1)))),
                          norm = ifelse(Variable == "x",
                                        exp(-.5*(((val-4)/5)^2))/(5*sqrt(2*pi)),
                                        exp(-.5*(((val-0)/1)^2))/(1*sqrt(2*pi)))),
            aes(y = norm), size = 1, color = "#014D64", alpha = .6) + 
  facet_wrap(~Variable, scales = "free",strip.position = "bottom") + 
  theme(axis.title = element_blank(),
        legend.justification = "center",
        strip.placement = "outside")

```

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>Same thing for the <b>coefficients</b> of the relationship between \(x\) and \(y\):</li>
</ul>

--

```{r}
lm(y ~ x, dt)
```

--

<p style = "margin-bottom:1.5cm"></p>

<ul>
  <li>But what would happen if we were to <b>redo this operation many times?</b></li>
  <ol>
    <li><b>Draw a random sample</b> from the DGP</li>
    <li><b>Compute the slope</b> of the regression of \(y\) on \(x\)</li>
    <li>Do it many times and <b>store the coefficients</b></li>
</ul>

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>We can <b>use a loop</b> to do that:</li>
  <ul>
    <li></li>
    <li></li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1cm"></p>

```{r}
#

for (i in 1:1000) {
  
#
# 
# 
  
# 
  
#
  
}
```
---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>We can <b>use a loop</b> to do that:</li>
  <ul>
    <li>First we create an empty vector</li>
    <li></li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1cm"></p>

```{r}
beta <- c()

for (i in 1:1000) {
  
#
# 
# 
  
# 
  
#
  
}
```

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>We can <b>use a loop</b> to do that:</li>
  <ul>
    <li>First we create an empty vector</li>
    <li>Then we put the code in a loop</li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1cm"></p>

```{r}
beta <- c()

for (i in 1:1000) {
  
  dt_i <- tibble(x = rnorm(1000, 4, 5),
                 e = rnorm(1000, 0, 1),
                 y = -2 + (.4 * x) + e)
  
  reg_i <- lm(y ~ x, dt_i)
  
#
  
}
```

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>We can <b>use a loop</b> to do that:</li>
  <ul>
    <li>First we create an empty vector</li>
    <li>Then we put the code in a loop</li>
    <li>And we fill the vector at each iteration</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm"></p>

```{r}
beta <- c()

for (i in 1:1000) {
  
  dt_i <- tibble(x = rnorm(1000, 4, 5),
                 e = rnorm(1000, 0, 1),
                 y = -2 + (.4 * x) + e)
  
  reg_i <- lm(y ~ x, dt_i)
  
  beta <- c(beta, reg_i$coefficients[2])
  
}
```

---

### 1. Asymptotic inference

#### 1.1. Data generating process

<ul>
  <li>We now have <b>1,000 slope coefficients</b> from 1,000 random samples of the <b>same DGP</b></li>
</ul>

.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4.5, out.width = "95%", fig.align='left'}
ggplot(tibble(beta = beta), aes(x = beta)) + 
  geom_histogram(fill = "#6794A7", color = "#014D64", alpha = .8) +
  geom_vline(xintercept = mean(beta), linetype = "dashed", color = "#014D64")
```
]

--

.right.column[

<p style = "margin-bottom:1.25cm"></p>

<ul style = "text-align:left;">
  <li style = "margin-bottom:1cm">Some random samples give higher estimates than others</li>
  <li style = "margin-bottom:1cm">But <b>on expectation</b> we get the <b>right coefficient!</b></li>
  <li style = "margin-bottom:1cm">The \(\hat{\beta}\)s actually follow a <b>normal distribution</b></li>
  <li style = "margin-bottom:1cm">And at the limit their mean would <b>converge towards \(\beta\)</b></li>
</ul>
]

---

### 1. Asymptotic inference

#### 1.2. Standardization

<ul>
  <li style = "margin-bottom:.12cm">That is crucial information because it allows to get back to something we know:</li>
  <ul>
    <li style = "margin-bottom:.12cm"></li>
    <li></li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
distrib <- tibble(beta = beta,
                  beta_demean_temp = beta - .2,
                  beta_demean = beta - .4,
                  beta_norm_temp = beta_demean / .3,
                  beta_norm = beta_demean / sd(beta))

norm_dt <- tibble(x = seq(-3, 3, .001),
                  y = (exp(-(x^2)/2)/sqrt(2*pi)))

distrib %>%
  ggplot(aes(x = beta)) +  
  geom_density(fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
]
.right-column[
<p style = "margin-bottom:1.25cm"></p>
$$\hat{\beta}$$
]

---

### 1. Asymptotic inference

#### 1.2. Standardization

<ul>
  <li>That is crucial information because it allows to get back to something we know:</li>
  <ul>
    <li style = "margin-bottom:.12cm">By subtracting \(\beta\) from the distribution of \(\hat{\beta}\)</li>
    <li></li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
distrib %>%
  pivot_longer(c(beta, beta_demean_temp, beta_demean), 
               names_to = "distrib", values_to = "val") %>%
  ggplot(aes(x = val, group = distrib)) + 
  annotate("segment", x = .35, xend = .25, y = 40, yend = 40,
           colour = "#6794A7", size = 1, arrow = arrow()) +
 annotate("segment", x = .15, xend = .05, y = 40, yend = 40,
           colour = "#6794A7", size = 1, arrow = arrow()) +
  geom_density(fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
]

.right-column[
<p style = "margin-bottom:1.25cm"></p>
$$\hat{\beta}-\beta$$
]

---

### 1. Asymptotic inference

#### 1.2. Standardization

<ul>
  <li>That is crucial information because it allows to get back to something we know:</li>
  <ul>
    <li>By subtracting \(\beta\) from the distribution of \(\hat{\beta}\)</li>
    <li>And dividing by the standard deviation of \(\hat{\beta}\)</li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
distrib %>%
  pivot_longer(c(beta_demean, beta_norm_temp, beta_norm), 
               names_to = "distrib", values_to = "val") %>%
  filter(val > -1 & val < 1) %>%
  ggplot(aes(x = val, group = distrib)) + 
  annotate("segment", x = -.05, xend = -.1, y = 40, yend = 20,
           colour = "#6794A7", size = 1, arrow = arrow()) +
  annotate("segment", x = .05, xend = .1, y = 40, yend = 20,
           colour = "#6794A7", size = 1, arrow = arrow()) +
  annotate("segment", x = -.15, xend = -.45, y = 15, yend = 5,
           colour = "#6794A7", size = 1, arrow = arrow()) +
  annotate("segment", x = .15, xend = .45, y = 15, yend = 5,
           colour = "#6794A7", size = 1, arrow = arrow()) +
  geom_density(fill = "#6794A7", color = "#014D64", alpha = .4) +
  theme(axis.title = element_blank())
```
]

.right-column[
<p style = "margin-bottom:1.25cm"></p>
$$\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}$$
]

---

### 1. Asymptotic inference

#### 1.2. Standardization

<ul>
  <li>That is crucial information because it allows to get back to something we know:</li>
  <ul>
    <li>By subtracting \(\beta\) from the distribution of \(\hat{\beta}\)</li>
    <li>And dividing by the standard deviation of \(\hat{\beta}\)</li>
    <li>With an infinite sample we would obtain the standard normal distribution</li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(distrib, aes(x = beta_norm)) + 
  geom_line(data = norm_dt, aes(x = x, y = y), color = "black",
            size = 1.2, linetype = "dashed")  + 
  geom_density(fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
]


.right-column[
<p style = "margin-bottom:1.25cm"></p>
$$\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
]

---

### 1. Asymptotic inference

#### 1.3. Confidence interval

<ul>
  <li>We can use the fact that we know the standard normal distribution:</li>
  <ul>
    <li></li>
    <li></li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(norm_dt, aes(x = x, y = y)) + 
  geom_line() +
  theme(axis.title = element_blank())
```
<p style = "margin-top:-8cm;margin-left:17.5cm">
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
]

---

### 1. Asymptotic inference

#### 1.3. Confidence interval

<ul>
  <li>We can use the fact that we know the standard normal distribution:</li>
  <ul>
    <li>That 99% of the distribution lie between \(\pm\) 2.58</li>
    <li></li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(norm_dt, aes(x = x, y = y)) + 
  geom_line() +
  #geom_segment(aes(x = -2.58, xend = -2.58, y = 0, 
  #                 yend = y[match(-2.58, x)]), 
  #             linetype = "dashed") +
  #geom_segment(aes(x = 2.58, xend = 2.58, y = 0, 
  #                 yend = y[match(2.58, x)]), 
  #             linetype = "dashed") +
  annotate("text", x = -2.75, y = .03, label = "0.5%") + 
  annotate("text", x = 2.75, y = .03, label = "0.5%") + 
  annotate("text", x = 0, y = .17, label = "99%") + 
  scale_x_continuous(breaks = c(-3, -2.58, -2,  -1, 0, 1, 2, 2.58, 3)) +
  geom_area(data = norm_dt %>% filter(x < -2.58), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = norm_dt %>% filter(x > 2.58), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
<p style = "margin-top:-8cm;margin-left:17.5cm">
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
$$\text{Pr}\left[-2.58<\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}<2.58\right] \approx 99\%$$
]


---

### 1. Asymptotic inference

#### 1.3. Confidence interval

<ul>
  <li>We can use the fact that we know the standard normal distribution:</li>
  <ul>
    <li>That 99% of the distribution lie between \(\pm\) 2.58</li>
    <li>That 95% of the distribution lie between \(\pm\) 1.96</li>
    <li></li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(norm_dt, aes(x = x, y = y)) + 
  geom_line() +
  #geom_segment(aes(x = -1.96, xend = -1.96, y = 0, 
  #                 yend = y[match(-1.96, x)]), 
  #             linetype = "dashed") +
  #geom_segment(aes(x = 1.96, xend = 1.96, y = 0, 
  #                 yend = y[match(1.96, x)]), 
  #             linetype = "dashed") +
  annotate("text", x = -2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 0, y = .17, label = "95%") + 
  scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3)) +
  geom_area(data = norm_dt %>% filter(x < -1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = norm_dt %>% filter(x > 1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```

<p style = "margin-top:-8cm;margin-left:17.5cm">
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
$$\text{Pr}\left[-1.96<\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}<1.96\right] \approx 95\%$$
]

---

### 1. Asymptotic inference

#### 1.3. Confidence interval

<ul>
  <li>We can use the fact that we know the standard normal distribution:</li>
  <ul>
    <li>That 99% of the distribution lie between \(\pm\) 2.58</li>
    <li>That 95% of the distribution lie between \(\pm\) 1.96</li>
    <li>This is what allows to determine confidence intervals</li>
  </ul>
</ul>
    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(norm_dt, aes(x = x, y = y)) + 
  geom_line() +
  #geom_segment(aes(x = -1.96, xend = -1.96, y = 0, 
  #                 yend = y[match(-1.96, x)]), 
  #             linetype = "dashed") +
  #geom_segment(aes(x = 1.96, xend = 1.96, y = 0, 
  #                 yend = y[match(1.96, x)]), 
  #             linetype = "dashed") +
  annotate("text", x = -2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 0, y = .17, label = "95%") + 
  scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3)) +
  geom_area(data = norm_dt %>% filter(x < -1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = norm_dt %>% filter(x > 1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```


<p style = "margin-top:-8cm;margin-left:17.5cm">
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
$$\text{Pr}\left[-1.96<\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}<1.96\right] \approx 95\%$$
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\:\Downarrow$$
$$\:\:\:\:\:\:\:\:\:\:\:\:\:\:\text{Confidence interval}$$
]

---

### 1. Asymptotic inference

#### 1.3. Confidence interval

$$\text{Pr}\left[-1.96<\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}<1.96\right] \approx 95\%$$

--
<p style = "margin-bottom:1.25cm"></p>

$$\text{Pr}\left[-1.96\times\color{SkyBlue}{\text{SD}(\hat{\beta})}<\hat{\beta}-\beta<1.96\times\color{SkyBlue}{\text{SD}(\hat{\beta})}\right] \approx 95\%$$

--
<p style = "margin-bottom:1.25cm"></p>

$$\text{Pr}\left[-1.96\times\text{SD}(\hat{\beta})\color{SkyBlue}{-\hat{\beta}}<-\beta<1.96\times\text{SD}(\hat{\beta})\color{SkyBlue}{-\hat{\beta}}\right] \approx 95\%$$

--
<p style = "margin-bottom:1.25cm"></p>

$$\text{Pr}\left[\color{SkyBlue}{+}1.96\times\text{SD}(\hat{\beta}) \color{SkyBlue}{+} \hat{\beta}\color{SkyBlue}{>}\beta\color{SkyBlue}{>}\color{SkyBlue}{-}1.96\times\text{SD}(\hat{\beta})\color{SkyBlue}{+}\hat{\beta}\right] \approx 95\%$$

--
<p style = "margin-bottom:1.25cm"></p>

$$\text{CI}_{95\%}: \:\hat{\beta}\pm 1.96\times\text{SD}(\hat{\beta})$$
---

<h3>Overview</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

.pull-right[

<ul style = "margin-left:-1cm;list-style: none">
  <li><b>3. Hypothesis testing</b></li>
  <ul style = "list-style: none">
    <li>3.1. P-value</li>
    <li>3.2. linearHypothesis()</li>
  </ul>
</ul>
 
<p style = "margin-bottom:1.75cm;"></p>

<ul style = "margin-left:-1cm;list-style: none"><li><b>4. Wrap up!</b></li></ul>
]

---

<h3>Overview</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>In the <b>previous section</b> I used phrases like <i>"at the limit"</i> or <b><i>"with an infinite sample"</i></b></li>
  <ul>
    <li>But <b>in practice</b> this is <b>not the case</b>, so things behave slightly differently</li>
    <li>And this implies to make a few <b>statistical adjustments</b> to account for that</li>
  </ul>
</ul>

<p style = "margin-bottom:1.15cm;"></p>

--

$$\hat{\beta}\pm 1.96\times\color{SkyBlue}{\text{SD}(\hat{\beta})}$$

<p style = "margin-bottom:1.15cm;"></p>

<ul>
  <li>First we <b>cannot measure</b> directly the <b>standard deviation</b> of \(\hat{\beta}\)</li>
  <ul>
    <li>Indeed in practice we have <b>only one observation</b> of \(\hat{\beta}\), not its whole distribution</li>
    <li>But like for the mean, we can compute a <b>standard error</b> instead</li>
  </ul>
</ul>

--

<p style = "margin-bottom:1.15cm;"></p>

.pull-left[
<center><b>Standard deviation</b></center>
<p style = "margin-bottom:.5cm;"></p>
<center><i>&#10140; Measures the amount of variability, or dispersion, from the individual data values to the mean</i></center>
]

.pull-right[
<center><b>Standard error</b></center>
<p style = "margin-bottom:.5cm;"></p>
<center><i>&#10140; Measures how far an estimate from a given sample is likely to be from the true parameter of interest</i></center>
]

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>We won't go through the theoretical computations together, but let's have a look at the formula:</li>
</ul>

<p style = "margin-bottom:1cm;"></p>
 
$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(n-\#\text{parameters})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$

<p style = "margin-bottom:1.25cm;"></p>

--

<ul>
  <li>Notice that the variance, and thus the standard error of our estimate, decreases as:</li>
</ul>

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>We won't go through the theoretical computations together, but let's have a look at the formula:</li>
</ul>

<p style = "margin-bottom:1cm;"></p>
 
$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(\color{SkyBlue}{n}-\#\text{parameters})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>Notice that the variance, and thus the standard error of our estimate, decreases as:</li>
  <ul>
    <li>The <span style = "color:#87CEEB;">number of of observations</span> gets bigger</li>
  </ul>
</ul>

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>We won't go through the theoretical computations together, but let's have a look at the formula:</li>
</ul>

<p style = "margin-bottom:1cm;"></p>

$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(n-\color{SkyBlue}{\#\text{parameters}})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>Notice that the variance, and thus the standard error of our estimate, decreases as:</li>
  <ul>
    <li>The number of of observations gets bigger</li>
    <li>The <span style = "color:#87CEEB;">number of parameters</span> decreases</li>
  </ul>
</ul>

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>We won't go through the theoretical computations together, but let's have a look at the formula:</li>
</ul>

<p style = "margin-bottom:1cm;"></p>
 
$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\color{SkyBlue}{\sum_{i = 1}^n\hat{\varepsilon_i}^2}}{(n-\#\text{parameters})\color{SkyBlue}{\sum_{i = 1}^n(x_i-\bar{x})^2}}}$$

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>Notice that the variance, and thus the standard error of our estimate, decreases as:</li>
  <ul>
    <li>The number of of observations gets bigger</li>
    <li>The number of parameters decreases</li>
    <li>The <span style = "color:#87CEEB;">sum of squared errors relative to the variance of \(x\)</span> decreases</li>
  </ul>
</ul>

---

### 2. Exact inference

#### 2.1. Standard error

<ul>
  <li>We won't go through the theoretical computations together, but let's have a look at the formula:</li>
</ul>

<p style = "margin-bottom:1cm;"></p>
 
$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(n-\#\text{parameters})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>Notice that the variance, and thus the standard error of our estimate, decreases as:</li>
  <ul>
    <li>The number of of observations gets bigger</li>
    <li>The number of parameters decreases</li>
    <li>The sum of squared errors decreases relative to the variance of \(x\)</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul>
  <li>And as the <b>standard error</b> gets <b>bigger, the <b>confidence interval</b> gets <b>bigger</b>:</li>
<ul>

<p style = "margin-bottom:.75cm;"></p>

$$\hat{\beta}\pm 1.96\times\color{SkyBlue}{\text{se}(\hat{\beta})}$$

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>But that's not it, remember that we took the value <b>1.96</b> from the <b>normal distribution</b></li>
</ul>

--

<p style = "margin-bottom:.75cm;"></p>

$$\hat{\beta}\pm \color{SkyBlue}{1.96}\times\text{se}(\hat{\beta})$$
--

<p style = "margin-bottom:.75cm;"></p>

.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "80%", fig.align='left'}
ggplot(norm_dt, aes(x = x, y = y)) + 
  annotate("text", x = -2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 0, y = .17, label = "95%") + 
  scale_x_continuous(breaks = c(-3, -1.96, -1, 0, 1, 1.96, 3)) +
  geom_line() +
  geom_area(data = norm_dt %>% filter(x < -1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = norm_dt %>% filter(x > 1.96), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
]

--

.right-column[
<ul style = "margin-top:-.5cm;margin-left:-3.25cm;">
  <li style = "margin-bottom:.5cm">But the <b>normal distribution</b> is what \(\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})}\) converges to <i><b>at the limit</b></i></li>
  <li style = "margin-bottom:.5cm">In the <b>finite</b> world, \(\frac{\hat{\beta}-\beta}{\text{se}(\hat{\beta})}\) follows a slightly flatter distribution</li>
  <li>The <b>Student \(t\) distribution</b>, whose precise shape depends on the number of observations we have and parameters we estimate</li>
</ul>
]

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>The Student \(t\) distribution <b>accounts for</b> the fact that the <b>sample is finite</b></li>
  <ul>
    <li>The lower the number of <b>degrees of freedom</b> (#observations - #parameters) the flatter</li>
    <li>And it <b>tends to a normal</b> distribution as the number of degrees of freedom \(\rightarrow \infty\)</li>
  </ul>
</ul>

```{r, echo = F, fig.width = 8, fig.height = 3.5, out.width = "75%"}
tdistribs <- tibble(x = seq(-8, 8, .01),
               `1` = dt(x, 1),
               `5` = dt(x, 5),
               `60` = dt(x, 60)) %>%
  pivot_longer(-x, names_to = "df", values_to = "y")

ggplot(tdistribs, aes(x = x, y = y, color = df, fill = df)) + 
  geom_line(alpha = .5, show.legend = F) +
  geom_area(data = tdistribs %>% filter(df == 60), alpha = .3, fill = "#6794A7", color = NA) +
  geom_area(data = tdistribs %>% filter(df == 5), alpha = .3, fill = "#00A2D9", color = NA) +
  geom_area(data = tdistribs %>% filter(df == 1), alpha = .3, fill = "#014D64", color = NA) +
  geom_area(data = tdistribs %>% mutate(y = NA_real_), alpha = .3) +
  theme(legend.direction = "vertical",
        legend.position = "right",
        axis.title = element_blank())
```

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>But <b>we know the Student</b> \(t\) <b>distributions</b> just as well as the standard normal distribution</li>
</ul>   

<p style = "margin-bottom:-.52cm"></p>

--
<ul>
  <ul>
    <li>With 100 degrees of freedom, 95% of the distribution lie between \(\pm\) 1.98</li>
    <li></li>
  </ul>
</ul>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="70%"}
stud_dt <- tibble(x = seq(-3, 3, .001),
                  y100 = dt(x, 100),
                  y3000 = dt(x, 3000))

ggplot(stud_dt, aes(x = x, y = y100)) + 
  geom_line() +
  annotate("text", x = -2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 2.5, y = .045, label = "2.5%") + 
  annotate("text", x = 0, y = .17, label = "95%") + 
  scale_x_continuous(breaks = c(-3, -1.98, -1, 0, 1, 1.98, 3)) +
  geom_area(data = stud_dt %>% filter(x < -1.98), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = stud_dt %>% filter(x > 1.98), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>But <b>we know the Student \(t\) distributions</b> just as well as the standard normal distribution</li>
  <ul>
    <li>With 100 degrees of freedom, 95% of the distribution lie between \(\pm\) 1.98</li>
    <li>With 3,000 degrees of freedom, 90% of the distribution lie between \(\pm\) 1.65</li>
  </ul>
</ul>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="70%"}
stud_dt <- tibble(x = seq(-3, 3, .001),
                  y100 = dt(x, 100),
                  y3000 = dt(x, 3000))

ggplot(stud_dt, aes(x = x, y = y100)) + 
  geom_line() +
  annotate("text", x = -2.5, y = .045, label = "5%") + 
  annotate("text", x = 2.5, y = .045, label = "5%") + 
  annotate("text", x = 0, y = .17, label = "90%") + 
  scale_x_continuous(breaks = c(-3, -2, -1.65, -1, 0, 1, 2, 1.65, 3)) +
  geom_area(data = stud_dt %>% filter(x < -1.65), fill = "#6794A7", color = "#014D64", alpha = .6) +
  geom_area(data = stud_dt %>% filter(x > 1.65), fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>So <b>instead of 1.96</b>, we must use the <b>value such that:</b></li>
  <ul>
    <li>The <b>desired percentage</b> of the distribution is comprised within \(\pm\) that value...</li>
    <li>For a Student \(t\) distribution with the relevant number of <b>degrees of freedom</b></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm;"></p>

--

<ul>
  <li>We can get these values easily with the <b>qt()</b> function, indicating:</li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>
 
```{r, eval = F}
qt( , )
```

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>So <b>instead of 1.96</b>, we must use the <b>value such that:</b></li>
  <ul>
    <li>The <b>desired percentage</b> of the distribution is comprised within \(\pm\) that value...</li>
    <li>For a Student \(t\) distribution with the relevant number of <b>degrees of freedom</b></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>We can get these values easily with the <b>qt()</b> function, indicating:</li>
  <ul>
    <li>The share of the distribution below the value we're looking for (e.g., 0.975 for a 95% CI)</li>
    <li></li>
  </ul>
</ul>
 
```{r, eval = F}
qt(.975, )
```

---

### 2. Exact inference

#### 2.2. Student-t distribution

<ul>
  <li>So <b>instead of 1.96</b>, we must use the <b>value such that:</b></li>
  <ul>
    <li>The <b>desired percentage</b> of the distribution is comprised within \(\pm\) that value...</li>
    <li>For a Student \(t\) distribution with the relevant number of <b>degrees of freedom</b></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>We can get these values easily with the <b>qt()</b> function, indicating:</li>
  <ul>
    <li>The share of the distribution below the value we're looking for (e.g., 0.975 for a 95% CI)</li>
    <li>The number of degrees of freedom of the Student \(t\) distribution (e.g., 88 observations - 2 parameters)</li>
  </ul>
</ul>
 
```{r, eval = F}
qt(.975, 86)
```

--
  
```{r, echo = F}
qt(.975, 86)
```

--

<p style = "margin-bottom:1.25cm;"></p>

<ul>
  <li>Denote this value \(t(\text{df})_{1-\frac{\alpha}{2}}\)</li>
  <ul>
    <li>With \(\alpha\) equal to \(1 -\) the confidence level</li>
    <li>And \(\text{df}\) the number of degrees of freedom</li>
  </ul>
</ul>

---

### 2. Exact inference

#### 2.3. Confidence interval
  
<ul>
  <li>The formula for the confidence interval in finite sample hence writes:</li>
</ul>

$$\hat{\beta}\pm \color{SkyBlue}{t(\text{df})_{1-\frac{\alpha}{2}}}\times\text{se}(\hat{\beta})$$

--

<ul>
  <li>The confidence interval increases as:</li>
  <ul>
    <li>The confidence level increases</li>
    <li></li>
  </ul>
</ul>

```{r, echo = F, fig.height = 3.25, fig.width = 8, out.width="62%"}
ggplot() + 
  geom_line(data = tdistribs %>% filter(df == 5), aes(x = x, y = y), color = "#014D64", alpha = .5, show.legend = F)  +
  geom_area(data = tdistribs %>% filter(df == 5), aes(x = x, y = y), alpha = .1, fill = "#014D64", color = NA) +
  geom_segment(data = tibble(q = c(.005, .025, .05, .95, .975, .995),
                             group = c(1, 2, 3, 3, 2, 1)) %>%
                 mutate(x = qt(q, 5), y = 0, yend = dt(x, 5)),
               mapping = aes(x = x, xend = x, y = y, yend = yend, color = as.factor(group)), show.legend = F, linetype = "dashed") +
  scale_y_continuous(limits = c(0, .42)) + 
  theme(legend.direction = "vertical",
        legend.position = "right",
        axis.title = element_blank())
```

---

### 2. Exact inference

#### 2.3. Confidence interval
  
<ul>
  <li>The formula for the confidence interval in finite sample hence writes:</li>
</ul>

$$\hat{\beta}\pm \color{SkyBlue}{t(\text{df})_{1-\frac{\alpha}{2}}}\times\text{se}(\hat{\beta})$$

<ul>
  <li>The confidence interval increases as:</li>
  <ul>
    <li>The confidence level increases</li>
    <li>The number of degrees of freedom decreases</li>
  </ul>
</ul>

```{r, echo = F, fig.height = 3.25, fig.width = 8, out.width="62%"}
ggplot() + 
  geom_line(data = tdistribs, aes(x = x, y = y, color = df, fill = df), alpha = .5, show.legend = F)  +
  geom_area(data = tdistribs %>% filter(df == 60), aes(x = x, y = y), alpha = .1, fill = "#6794A7", color = NA) +
  geom_area(data = tdistribs %>% filter(df == 5), aes(x = x, y = y), alpha = .1, fill = "#00A2D9", color = NA) +
  geom_area(data = tdistribs %>% filter(df == 1), aes(x = x, y = y), alpha = .1, fill = "#014D64", color = NA) +
  #geom_area(data = tdistribs %>% mutate(y = NA_real_), aes(x = x, y = y, color = df, fill = df), alpha = .3) +
  geom_segment(data = tibble(q = rep(c(.05, .95), each = 3),
                             df = rep(c(1, 5, 60), 2)) %>%
                 mutate(x = qt(q, df), y = 0, yend = dt(x, df)),
               mapping = aes(x = x, xend = x, y = y, yend = yend, color = as.factor(df)), show.legend = F, linetype = "dashed") +
  scale_y_continuous(limits = c(0, .42)) + 
  theme(legend.direction = "vertical",
        legend.position = "right",
        axis.title = element_blank())
```


---

class: inverse, hide-logo

### Practice

#### 1) Import the `ggcurve.csv` dataset

--

<p style = "margin-bottom:1cm;"></p>

#### 2) Regress the IGE on the Gini coefficient and store the estimated regression parameters

--

<p style = "margin-bottom:1cm;"></p>

#### 3) Compute the 95% confidence interval of the regression slope


<p style = "margin-bottom:1cm;"></p>

$$\hat{\beta}\pm t(\text{df})_{1-\frac{\alpha}{2}}\times\text{se}(\hat{\beta})$$

<p style = "margin-bottom:1cm;"></p>

$$\text{se}(\hat{\beta}) = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(n-\#\text{parameters})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$
--

<p style = "margin-bottom:1.5cm;"></p>

<center><h3><i>You've got 10 minutes!</i></h3></center>

`r countdown(minutes = 10, top = 0, right = 0, play_sound = F, color_border = "#DFE6EB", color_text = "#DFE6EB", color_running_background = "#DFE6EB", color_running_text = "#014D64", color_finished_background = "#014D64", color_finished_text = "#DFE6EB", start_immediately = T)`


---

class: inverse, hide-logo

### Solution

#### 1) Import the `ggcurve.csv` dataset

--

```{r, eval= F }
ggcurve <- read.csv("C:/User/Documents/ggcurve.csv")
```

```{r, echo= F }
ggcurve <- read.csv("ggcurve.csv")
```

--

#### 2) Regress the IGE on the Gini coefficient and store the regression slope

--

```{r}
model <- lm(ige ~ gini, ggcurve)
model
```

--

```{r}
alpha <- model$coefficients[1]
beta <- model$coefficients[2]
```


---

class: inverse, hide-logo

### Solution

#### 3) Compute the 95% confidence interval of the regression slope

```{r}
se_dat <- ggcurve %>%
  mutate(fit = alpha + gini * beta, e = ige - fit) %>%
  summarise(se = sqrt(sum(e^2)/((n()-2)*sum((gini-mean(gini))^2))))

se_dat$se
```

--

```{r}
beta - se_dat$se * qt(.975, nrow(ggcurve) - 2)
beta + se_dat$se * qt(.975, nrow(ggcurve) - 2)
```

---

<h3>Overview</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

.pull-right[

<ul style = "margin-left:-1cm;list-style: none">
  <li><b>3. Hypothesis testing</b></li>
  <ul style = "list-style: none">
    <li>3.1. P-value</li>
    <li>3.2. linearHypothesis()</li>
  </ul>
</ul>
 
<p style = "margin-bottom:1.75cm;"></p>

<ul style = "margin-left:-1cm;list-style: none"><li><b>4. Wrap up!</b></li></ul>
]

---

<h3>Overview</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

.pull-right[

<ul style = "margin-left:-1cm;list-style: none">
  <li><b>3. Hypothesis testing</b></li>
  <ul style = "list-style: none">
    <li>3.1. P-value</li>
    <li>3.2. linearHypothesis()</li>
  </ul>
</ul>

]

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We now have the <b>95% confidence interval</b> for our estimate:</li>
  <ul>
    <li>Our estimate of \(\beta\) is 1.02</li>
    <li>And we are 95% sure that \(\beta\) lies between 0.46 and 1.57</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

$$0.46 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, 1.02 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, 1.57\\
|\underbrace{-----------}_{t(\text{df})_{1-\frac{\alpha}{2}}\times\text{se}(\hat{\beta})}\cdot\underbrace{-----------}_{t(\text{df})_{1-\frac{\alpha}{2}}\times\text{se}(\hat{\beta})}|$$

--

<p style = "margin-bottom:1cm;"></p>

<ul>
  <li>Note that in our <b>confidence interval</b> formula:</li>
  <ul>
    <li>The <b>standard error</b> and the relevant Student \(t\) <b>distribution</b> are <b>given</b></li>
    <li>But the <b>confidence level</b> \(1 - \alpha\) was <b>chosen arbitrarily</b></li>
  </ul>
</ul>

--

<p style = "margin-bottom:1cm;"></p>

&#10140; Setting a <b>higher confidence</b> level would <b>widen the confidence interval</b>  
&#10140; Allowing for a <b>lower confidence</b> level would <b>narrow the confidence interval</b>

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>So far we framed the problem as:</li>
</ul>

<center><i>"What are the values \(\beta\) is likely to take under a given confidence level?"</i></center>

<p style = "margin-bottom:1.5cm;"></p>

--

<ul>
  <li>But we could also think of it as:</li>
</ul>

<center><i>"Under which confidence level is \(\beta\) is likely to take a given value?"</i></center>

--

<p style = "margin-bottom:1.5cm;"></p>

<ul>
  <li>And this is actually a very <b>practical way of framing the question:</b></li>
  <ul>
    <li>To (in)validate the predictions from a theoretical model</li>
    <li>To know under which confidence level \(\beta\) is likely to be \(\neq 0\) at all</li>
  </ul>
</ul>

--

<p style = "margin-bottom:1.5cm;"></p>

<center><b><i>&#10140; But how to answer such questions in practice?</i></b></center>

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We can start from the fact that even though we do not know \(\beta\), we know that:</li>
</ul>

$$\frac{\hat{\beta} - \beta}{\text{se}(\hat{\beta})} \sim t(\text{df}) $$

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot() + 
  geom_line(data = stud_dt %>%
         mutate(zero = 0, n = row_number()) %>%
         pivot_longer(c(zero, y3000), values_to = "y", names_to = "osef") %>%
         group_by(n) %>%
         mutate(`Prob:` = max(y)), 
       aes(x = x, y = y, group = n, color = `Prob:`)) +
  geom_line(data = stud_dt, 
            aes(x = x, y = y3000), 
            color = "#014D64", alpha = .1) +
  scale_color_gradient(low = "#DFE6EB", high = "#014D64", 
                       breaks = c(min(stud_dt$y3000), max(stud_dt$y3000)), 
                       labels = c("", "")) + 
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```


---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>And that in this distribution some values are quite plausible:</li>
</ul>

$$\frac{\hat{\beta} - \color{SkyBlue}{1}}{\text{se}(\hat{\beta})}$$

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot() + 
  geom_line(data = stud_dt %>%
         mutate(zero = 0, n = row_number()) %>%
         pivot_longer(c(zero, y3000), values_to = "y", names_to = "osef") %>%
         group_by(n) %>%
         mutate(`Prob:` = max(y)), 
       aes(x = x, y = y, group = n, color = `Prob:`)) +
  geom_line(data = stud_dt, 
            aes(x = x, y = y3000), 
            color = "#014D64", alpha = .1) +
  geom_segment(aes(x = beta - 1, xend = beta - 1,
                   y = 0, yend = stud_dt$y3000[match(round(beta - 1, 3), round(stud_dt$x, 3))]), 
               linetype = "dashed", color = "cyan") +
  scale_color_gradient(low = "#DFE6EB", high = "#014D64", 
                       breaks = c(min(stud_dt$y3000), max(stud_dt$y3000)), 
                       labels = c("", "")) + 
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>And some are way less plausible:</li>
</ul>

$$\frac{\hat{\beta} - \color{SkyBlue}{2.5}}{\text{se}(\hat{\beta})}$$

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot() + 
  geom_line(data = stud_dt %>%
         mutate(zero = 0, n = row_number()) %>%
         pivot_longer(c(zero, y3000), values_to = "y", names_to = "osef") %>%
         group_by(n) %>%
         mutate(`Prob:` = max(y)), 
       aes(x = x, y = y, group = n, color = `Prob:`)) +
  geom_line(data = stud_dt, 
            aes(x = x, y = y3000), 
            color = "#014D64", alpha = .1) +
  geom_segment(aes(x = beta - 2.5, xend = beta - 2.5,
                   y = 0, yend = stud_dt$y3000[match(round(beta - 2.5, 3), round(stud_dt$x, 3))]), 
               linetype = "dashed", color = "cyan") +
  scale_color_gradient(low = "#DFE6EB", high = "#014D64", 
                       breaks = c(min(stud_dt$y3000), max(stud_dt$y3000)), 
                       labels = c("", "")) + 
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But because the distribution is <b>continuous:</b></li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But because the distribution is <b>continuous:</b></li>
  <ul>
    <li>The <b>probability</b> to draw any <b>exact value</b> would be <b>0</b></li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = 1.5, xend = 1.5,
                   y = 0, yend = y3000[match(round(1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But because the distribution is <b>continuous:</b></li>
  <ul>
    <li>The <b>probability</b> to draw any <b>exact value</b> would be <b>0</b></li>
    <li>We can only compute the <b>probability to fall below</b> that value</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = 1.5, xend = 1.5,
                   y = 0, yend = y3000[match(round(1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_area(data = stud_dt %>% filter(x <= 1.5), fill = "#014D64", color = NA, alpha= .2) +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But because the distribution is <b>continuous:</b></li>
  <ul>
    <li>The <b>probability</b> to draw any <b>exact value</b> would be <b>0</b></li>
    <li><b>Or to fall above</b> that value <b>if</b> it is <b>negative</b></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = -1.5, xend = -1.5,
                   y = 0, yend = y3000[match(round(-1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_area(data = stud_dt %>% filter(x >= -1.5), fill = "#014D64", color = NA, alpha= .2) +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But <b>generally</b> what makes sense is to know what are the <b>chances to fall <i>that far</i> from 0:</b></li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = -1.5, xend = -1.5,
                   y = 0, yend = y3000[match(round(-1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_segment(aes(x = 1.5, xend = 1.5,
                   y = 0, yend = y3000[match(round(1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But <b>generally</b> what makes sense is to know what are the <b>chances to fall <i>that far</i> from 0:</b></li>
  <ul>
    <li>So we take 1 - the probability to fall below the absolute value</li>
    <li></li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = -1.5, xend = -1.5,
                   y = 0, yend = y3000[match(round(-1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_segment(aes(x = 1.5, xend = 1.5,
                   y = 0, yend = y3000[match(round(1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_area(data = stud_dt %>% filter(x >= 1.5), fill = "#014D64", color = NA, alpha= .2) +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>But <b>generally</b> what makes sense is to know what are the <b>chances to fall <i>that far</i> from 0:</b></li>
  <ul>
    <li>So we take 1 - the probability to fall below the absolute value</li>
    <li>And we multiply it by 2</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

```{r, echo = F, fig.height = 3.75, fig.width = 8, out.width="65%"}
ggplot(data = stud_dt, aes(x = x, y = y3000)) + 
  geom_line(color = "#014D64", alpha = .6) +
  geom_segment(aes(x = -1.5, xend = -1.5,
                   y = 0, yend = y3000[match(round(-1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_segment(aes(x = 1.5, xend = 1.5,
                   y = 0, yend = y3000[match(round(1.5, 3), round(x, 3))]), 
               linetype = "dashed", color = "#014D64") +
  geom_area(data = stud_dt %>% filter(x >= 1.5), fill = "#014D64", color = NA, alpha= .2) +
  geom_area(data = stud_dt %>% filter(x <= -1.5), fill = "#014D64", color = NA, alpha= .2) +
  theme(axis.title = element_blank(),
        legend.position = "right",
        legend.direction = "vertical")
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>The <b>resulting area</b> is what we call a <b>p-value</b></li>
  <ul>
    <li>It is the probability that \(\beta\) falls at least as far from \(\hat{\beta}\) as the hypothesized value</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

--

<ul>
  <li>Consider finding \(\hat{\beta} = 4\) and a hypothesizing value of 3 for \(\beta\)</li>
  <ul>
    <li>A p-value of 5% indicates that there is only a 5% chance to find a \(\hat{\beta} = 4\) if \(\beta = 3\)</li>
    <li>Below that threshold we would reject the hypothesis that \(\beta = 3\) at the 95% confidence level</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

--

<ul>
  <li>Notice that in this example, the 95% confidence interval of \(\hat{\beta}\) would not include the value 3</li>
  <ul>
    <li>With a hypothesized value equal to the bound of a confidence interval the p-value would equal 1 - the corresponding confidence level</li>
    <li>So a p-value lower than \(\alpha\) means that the hypothesized value is outside the \((1-\alpha)\)% confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

<center><i>&#10140; Let's go through a formal example with our data</i></center>

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>Can we <b>reject</b> at the 95% confidence level that \(\beta = 0\)?</li>
</ul>

```{r}
beta
```

--

<ul>
  <li>We should start by hypothesizing that \(\beta = 0\)</li>
  <ul>
    <li>This is what we call the <b><i>"null hypothesis"</i></b> \(H_0\)</li>
  </ul>
</ul>

<p style = "margin-bottom:1.25cm"></p>

$$H_0: \beta = 0$$

--

<center>Under \(H_0\):</center>

$$\frac{\hat{\beta} - 0}{\text{se}(\hat{\beta})} \sim t(\text{df})$$


---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We should find the <b>area below</b> \((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) in a Student \(t\) distribution we the right number of \(\text{df}\)</li>
  <ul>
    <li>\((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) is what we call <b>the \(t\)-stat</b></li>
  </ul>
</ul>

--

```{r}
(beta - 0) / se_dat$se
```

<p style = "margin-bottom:1.25cm"></p>

--

<ul>
  <li>While <b>qt()</b> gave us the <b>value</b> for a certain probability, <b>pt()</b> gives the the <b>probability</b> for a given value:</li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>

```{r, eval = F}
pt( , )
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We should find the <b>area below</b> \((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) in a Student \(t\) distribution we the right number of \(\text{df}\)</li>
  <ul>
    <li>\((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) is what we call <b>the \(t\)-stat</b></li>
  </ul>
</ul>


```{r}
(beta - 0) / se_dat$se
```

<p style = "margin-bottom:1.25cm"></p>

<ul>
  <li>While <b>qt()</b> gave us the <b>value</b> for a certain probability, <b>pt()</b> gives the the <b>probability</b> for a given value:</li>
  <ul>
    <li>Put in the <b>t-stat</b></li>
    <li></li>
  </ul>
</ul>

```{r, eval = F}
pt((beta - 0) / se_dat$se, )
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We should find the <b>area below</b> \((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) in a Student \(t\) distribution we the right number of \(\text{df}\)</li>
  <ul>
    <li>\((\hat{\beta} - 0)/\text{se}(\hat{\beta})\) is what we call <b>the \(t\)-stat</b></li>
  </ul>
</ul>


```{r}
(beta - 0) / se_dat$se
```

<p style = "margin-bottom:1.25cm"></p>

<ul>
  <li>While <b>qt()</b> gave us the <b>value</b> for a certain probability, <b>pt()</b> gives the the <b>probability</b> for a given value:</li>
  <ul>
    <li>Put in the <b>t-stat</b></li>
    <li>And the <b>degrees of freedom</b></li>
  </ul>
</ul>

```{r}
pt((beta - 0) / se_dat$se, nrow(ggcurve) - 2)
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We must then:</li>
  <ul>
    <li>Take <b>1 - this probability</b> (area above the t-stat)</li>
    <li></li>
  </ul>
</ul>

```{r}
1 - pt(abs((beta - 0) / se_dat$se), nrow(ggcurve) - 2)
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>We must then:</li>
  <ul>
    <li>Take <b>1 - this probability</b> (area above the t-stat)</li>
    <li>And <b>multiply it by 2</b> (consider the absolute distance and not the signed distance)</li>
  </ul>
</ul>

```{r}
2 * (1 - pt(abs((beta - 0) / se_dat$se), nrow(ggcurve) - 2))
```

--

<p style = "margin-bottom:1cm"></p>

<ul>
  <li>The <b>p-value</b> is <b>lower than 1%:</b></li>
  <ul>
    <li>We can <b>reject at the 99% confidence level</b> that \(\beta = 0\)</li>
    <li>In that case we say that \(\hat{\beta}\) is <b>significantly different from 0</b> at the 1% significance level</li>
  </ul>
</ul>

--

<p style = "margin-bottom:1cm"></p>

<ul>
  <li>But the <b>p-value</b> is <b>greater than 0.1%:</b></li>
  <ul>
    <li>We <b>cannot reject at the 99.9% confidence level</b> that \(\beta = 0\)</li>
    <li>In that case we say that \(\hat{\beta}\) is <b>not significantly different from 0</b> at the 0.1% significance level</li>
  </ul>
</ul>

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>By default, the <b>summary()</b> function <b>tests</b> whether or not each coefficient is significantly <b>different from 0</b></li>
  <ul>
    <li></li>
  </ul>
</ul>

```{r, eval = F}
summary(lm(ige ~ gini, ggcurve))
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<ul>
  <li>By default, the <b>summary()</b> function <b>tests</b> whether or not each coefficient is significantly <b>different from 0</b></li>
  <ul>
    <li>You can <b>extract</b> the information from the <b>$coefficient</b> attribute of the output</li>
  </ul>
</ul>

```{r}
summary(lm(ige ~ gini, ggcurve))$coefficients
```

--

<p style = "margin-bottom:.8cm"></p>

<ul>
  <li>For each coefficient it indicates:</li>
  <ul>
    <li>The standard error</li>
    <li>The \(t\)-stat \((H_0:\beta=0)\)</li>
    <li>The p-value \((H_0:\beta=0)\)</li>
  </ul>
</ul>

--

<ul>
  <li>The output of the <b>summary()</b> function is great to have a <b>quick overview</b> of the model:</li>
</ul>

```{r, eval = F}
summary(lm(ige ~ gini, ggcurve))
```

---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

.right-column[
<b>&#x1F804;</b> Command  

]

---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

.right-column[
<b>&#x1F804;</b> Command  
<p style = "margin-bottom:1.5cm;"></p>
<b>&#x1F804;</b> Residuals distribution  

]

---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

.right-column[
<b>&#x1F804;</b> Command  
<p style = "margin-bottom:1.5cm;"></p>
<b>&#x1F804;</b> Residuals distribution  
<p style = "margin-bottom:1.75cm;"></p>
<b>&#x1F804;</b> Coefs, s.e., t-/p-values  
]

---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

.right-column[
<b>&#x1F804;</b> Command  
<p style = "margin-bottom:1.5cm;"></p>
<b>&#x1F804;</b> Residuals distribution  
<p style = "margin-bottom:1.75cm;"></p>
<b>&#x1F804;</b> Coefs, s.e., t-/p-values  
<p style = "margin-bottom:1.25cm;"></p>
<b>&#x1F804;</b> Significance  
]
---

### 3. Hypothesis testing

#### 3.1. P-value

<p style = "margin-bottom:1.25cm"></p>

.left-column[

<p style = "margin-bottom:-.53cm;"></p>
```{r, echo = F}
summary(lm(ige ~ gini, ggcurve))
```
]

.right-column[
<b>&#x1F804;</b> Command  
<p style = "margin-bottom:1.5cm;"></p>
<b>&#x1F804;</b> Residuals distribution  
<p style = "margin-bottom:1.75cm;"></p>
<b>&#x1F804;</b> Coefs, s.e., t-/p-values  
<p style = "margin-bottom:1.25cm;"></p>
<b>&#x1F804;</b> Significance  
<p style = "margin-bottom:1cm;"></p>
<b>&#x1F804;</b> df and advanced stats

]

---

### 3. Hypothesis testing

#### 3.2. linearHypothesis()

<ul>
  <li>But the <b>linearHypothesis()</b> function from the <b>car</b> package allows to <b>easily test</b> other <b>hypotheses:</b></li>
  <ul>
    <li></li>
    <li></li>
  </ul>
</ul>

```{r, echo = F}
library(car)
```

```{r, eval = F}
linearHypothesis( , )
```

---

### 3. Hypothesis testing

#### 3.2. linearHypothesis()

<ul>
  <li>But the <b>linearHypothesis()</b> function from the <b>car</b> package allows to <b>easily test</b> other <b>hypotheses:</b></li>
  <ul>
    <li>You must provide the <b>model</b></li>
    <li></li>
  </ul>
</ul>

```{r, eval = F}
linearHypothesis(lm(ige ~ gini, ggcurve), )
```

---

### 3. Hypothesis testing

#### 3.2. linearHypothesis()

<ul>
  <li>But the <b>linearHypothesis()</b> function from the <b>car</b> package allows to <b>easily test</b> other <b>hypotheses:</b></li>
  <ul>
    <li>You must provide the <b>model</b></li>
    <li>And the <b>hypothesis</b> (referring to coefficients as in the summary)</li>
  </ul>
</ul>

```{r, eval = F}
linearHypothesis(lm(ige ~ gini, ggcurve), "gini = 0")
```

--

```{r, echo = F}
linearHypothesis(lm(ige ~ gini, ggcurve), "gini = 0")
```
  
---

### 3. Hypothesis testing

#### 3.2. linearHypothesis()

<ul>
  <li>You can also test <b>more complex hypotheses</b></li>
  <ul>
    <li>Like equality between coefficients</li>
  </ul>
</ul>

```{r}
linearHypothesis(lm(ige ~ gini, ggcurve), "gini = (Intercept)")
```
  
---

### 3. Hypothesis testing

#### 3.2. linearHypothesis()

<ul>
  <li>You can also test <b>more complex hypotheses</b></li>
  <ul>
    <li>Like equality between coefficients, or joint hypotheses (relying a generalization of the t-test called <i>F-test</i>)</li>
  </ul>
</ul>

```{r}
linearHypothesis(lm(ige ~ gini, ggcurve), c("gini = 0", "(Intercept) = 0"))
```

---

<h3>Overview</h3>

<p style = "margin-bottom:3cm;"></p>

.pull-left[

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>1. Asymptotic inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>1.1. Data generating process</li>
    <li>1.2. Standardization</li>
    <li>1.3. Confidence interval</li>
  </ul>
</ul>

<p style = "margin-bottom:1cm;"></p>

<ul style = "margin-left:1.5cm;list-style: none">
  <li><b>2. Exact inference &#10004;</b></li>
  <ul style = "list-style: none">
    <li>2.1. Standard error</li>
    <li>2.2. Student-t distribution</li>
    <li>2.3. Confidence interval</li>
  </ul>
</ul>

]

.pull-right[

<ul style = "margin-left:-1cm;list-style: none">
  <li><b>3. Hypothesis testing &#10004;</b></li>
  <ul style = "list-style: none">
    <li>3.1. P-value</li>
    <li>3.2. linearHypothesis()</li>
  </ul>
</ul>
 
<p style = "margin-bottom:1.75cm;"></p>

<ul style = "margin-left:-1cm;list-style: none"><li><b>4. Wrap up!</b></li></ul>
]

---

### 4. Wrap up!

#### Data generating process

<ul>
  <li>In practice we estimate coefficients on a <b>given realization of a data generating process</b></li>
  <ul>
    <li>So the <b>true coefficient</b> is <b>unobserved</b></li>
    <li>But our <b>estimation</b> is <b>informative</b> on the values the true coefficient is likely to take</li>
  </ul>
</ul>

    
.left-column[
```{r, echo = F, fig.width = 8, fig.height = 4, out.width = "90%", fig.align='left'}
ggplot(distrib, aes(x = beta_norm)) + 
  geom_line(data = norm_dt, aes(x = x, y = y), color = "black",
            size = 1.2, linetype = "dashed")  + 
  geom_density(fill = "#6794A7", color = "#014D64", alpha = .6) +
  theme(axis.title = element_blank())
```
]


.right-column[
<p style = "margin-bottom:3cm"></p>
$$\frac{\hat{\beta}-\beta}{\text{SD}(\hat{\beta})} \sim \mathcal{N}(0, 1)$$
]

---

### 4. Wrap up!

#### Confidence interval

<ul>
  <li>This allows to infer a <b>confidence interval:</b></li>
</ul>

$$\hat{\beta}\pm t(\text{df})_{1-\frac{\alpha}{2}}\times\text{se}(\hat{\beta})$$

<p style = "margin-bottom:1.5cm;"></p>

--

<ul>
  <li>Where \(t(\text{df})_{1-\frac{\alpha}{2}}\) is the value from a <b>Student \(t\) distribution</b></li>
  <ul>
    <li>With the relevant number of <b>degrees of freedom</b> \(\text{df}\) (n - #parameters)</li>
    <li>And the desired <b>confidence level</b> \(1-\alpha\)</li>
  </ul>
</ul>

<p style = "margin-bottom:1.5cm;"></p>

--

<ul>
  <li>And where \(\text{se}(\hat{\beta})\) denotes the <b>standard error</b> of \(\hat{\beta}\):</li>
</ul>

$$\text{se}(\hat{\beta}) = \sqrt{\widehat{\text{Var}(\hat{\beta})}} = \sqrt{\frac{\sum_{i = 1}^n\hat{\varepsilon_i}^2}{(n-\#\text{parameters})\sum_{i = 1}^n(x_i-\bar{x})^2}}$$

---

### 4. Wrap up!

#### P-value

<ul>
  <li>It also allows to <b>test</b> how likely is \(\beta\) to be <b>different from a given value:</b></li>
  <ul>
    <li>If the <b>p-value</b> < 5%, we can <b>reject</b> that \(\beta\) equals the <b>hypothesized value</b> at the 95% confidence level</li>
    <li>This threshold, very common in Economics, implies that we have 1 chance out of 20 to be wrong</li>
  </ul>
</ul>

--

```{r}
linearHypothesis(lm(ige ~ gini, ggcurve), "gini = 0")
```

